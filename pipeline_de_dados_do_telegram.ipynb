{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31153,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introdução\n",
        "\n",
        "Neste projeto, será desenvolvido um pipeline de dados do Telegram. A proposta consiste em criar um grupo no Telegram cujas mensagens serão automaticamente coletadas e armazenadas em formato JSON em um bucket da AWS (Amazon Web Services).\n",
        "\n",
        "Após o armazenamento, esses dados serão processados e tratados, permitindo a realização de consultas SQL para análise e extração de insights.\n",
        "\n",
        "Esse pipeline serve como um exemplo prático de engenharia de dados aplicada a mensagens em tempo real, utilizando serviços em nuvem para coleta, armazenamento e análise.\n",
        "\n",
        "\n",
        "## Tecnologias utilizadas:\n",
        "1. AWS(Api gateway, s3, lambda...)\n",
        "2. Python.\n",
        "3. Sql.\n",
        "4. Webhooks.\n",
        "\n",
        "## Arquitetura do pipeline.\n",
        "A arquitetura é dividia em duas parte, o sistema transacional,feito no telegram, e o sistema analítico, feito no AWS. Abaixo, um esquema visual demonstrando o fluxo de dados do projeto:\n",
        "\n",
        "![Minha imagem](https://github.com/WlandGLL/imagens/blob/main/esquema.png?raw=true)\n",
        "  \n",
        "\n",
        "## Sistema transacional\n",
        "\n",
        "### Grupo do Telegram\n",
        "Será criado um grupo do telegram, e um bot será adicionado á ele.\n",
        "\n",
        "## Sistema analítico\n",
        "\n",
        "### Ingestão.\n",
        "\n",
        "As mensagens do grupo vão ser captadas pelo Api gateway, através de um wehbook, e encaminhadas a uma função lambda. Essa função vai transformar as mensagens em arquivos json e depois salvar em um bucket s3.\n",
        "\n",
        "### ETL.\n",
        "\n",
        "Será configurado um EventBridge, que acionará uma nova função Lambda sempre que um novo arquivo JSON for salvo no bucket. Essa função fará a transformação dos dados, convertendo o arquivo JSON em formato Parquet,e o salvará em outro bucket S3 otimizado para análise.\n",
        "\n",
        "### Apresentação.\n",
        "\n",
        "Por fim, será criada uma tabela particionada com os arquivos parquet, podendo fazer consultas sql e análise no AWS Athena.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mKwQKOzDPuZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Grupo.\n",
        "Nesta etapa, será criado um grupo e um bot. O bot será adicionado ao grupo como adminstrador e captará todas as mensagens enviadas. A opção de adicionar o bot a novos grupos será desabilitada e as mensagens que surgirem no grupo serão acessadas através da API (application programming interface) de bots dos Telegram (documentação neste link: https://core.telegram.org/bots/api).\n",
        "\n",
        "\n",
        "\n",
        "# 2.Ingestão\n",
        "A etapa de ingestão é responsável, como seu o próprio nome diz, pela ingestão dos dados  .De maneira geral, o dado ingerido é persistido no formato mais próximo do original, ou seja, nenhuma transformação é realizada em seu conteúdo ou estrutura (schema). Como exemplo, dados de uma API web que segue o formato REST (representational state transfer) são entregues, logo, persistidos, no formato JSON.\n",
        "\n",
        "\n",
        "## 2.1.AWS Api Gateway\n",
        "\n",
        "No AWS API gateway, será criada uma nova API HTTP, que servirá como ponte entre o Telegram e a função Lambda responsável pelo tratamento dos dados. Essa api, vai obter os dados através de um webhook\n",
        "\n",
        "![Minha imagem](https://github.com/WlandGLL/imagens/blob/main/111.png?raw=true)\n",
        "\n",
        "\n",
        "\n",
        "## Webhook\n",
        "\n",
        "Agora será configurado o webhook, para isso é necessário o token do bot que está no grupo, e a url de invocação disponibilizada pela API HTTP.\n",
        "\n",
        "### Token\n",
        "\n",
        "Para isso obter o token, no chat do BotFather(link:), digite:\n",
        "    "
      ],
      "metadata": {
        "id": "lQTmADdIPuZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "/token"
      ],
      "metadata": {
        "id": "kgR_nKRXPuZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ele responderá com o token do seu bot, algo como:"
      ],
      "metadata": {
        "id": "LMQros9YPuZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "123456789:ABCdefGhIjKlMnOpQrStUvWxYz"
      ],
      "metadata": {
        "id": "-QPAGX9yPuZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### URl da API\n",
        "Para obter a URl de invocação da API, vá em API Gateway → [sua API] → Stages → [stage] e copie a Invoke URL mostrada no topo.\n"
      ],
      "metadata": {
        "id": "NjqPVM3rPuZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Minha imagem](https://github.com/WlandGLL/imagens/blob/main/113.png?raw=true)\n"
      ],
      "metadata": {
        "id": "1Y0HjVOYPuZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuração\n",
        "Após obter o token, basta configurar o webhook digitando a seguinte URL no navegador (ou via terminal):\n",
        "\n",
        "https://api.telegram.org/botSEU_TOKEN/setWebhook?url=URL_DA_API_GATEWAY"
      ],
      "metadata": {
        "id": "43Tw_vfyPuZa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se o webhook for configurado corretamente, a resposta será"
      ],
      "metadata": {
        "id": "O1grIS4LPuZa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "{\n",
        "  \"ok\": true,\n",
        "  \"result\": true,\n",
        "  \"description\": \"Webhook was set\"\n",
        "}"
      ],
      "metadata": {
        "id": "kkr42D_ZPuZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. AWS Bucket S3\n",
        "Nesta etapa o AWS S3 tem a função de armazenar as mensagens enviadas no grupo passiavamente, no formato JSON. Para isso será criado um bucket S3.\n",
        "\n",
        "\n",
        "## 2.3. AWS lambda\n",
        "Depois de criar o bucket, o AWS Lambda tem a função de ativamente persistir as mensagens captadas pelo bot do Telegram em um bucket do AWS S3. Para tanto vamos criar uma função que opera da seguinte forma:\n",
        "\n",
        "Recebe a mensagem no parâmetro event;\n",
        "Verifica se a mensagem tem origem no grupo do Telegram correto;\n",
        "Persiste a mensagem no formato JSON no bucket do AWS S3;\n",
        "Retorna uma mensagem de sucesso (código de retorno HTTP igual a 200) a API de bots do Telegram.\n",
        "\n",
        "O código da função lambda:"
      ],
      "metadata": {
        "id": "MJBe2LobPuZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "from datetime import datetime, timezone, timedelta\n",
        "import boto3\n",
        "\n",
        "# Configura logs (para aparecer no CloudWatch)\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Variáveis de ambiente (definidas na AWS Lambda)\n",
        "BUCKET = os.environ['AWS_S3_BUCKET']\n",
        "TELEGRAM_CHAT_ID = int(os.environ['TELEGRAM_CHAT_ID'])\n",
        "\n",
        "# Cliente do S3\n",
        "s3_client = boto3.client('s3')\n",
        "\n",
        "def lambda_handler(event, context):\n",
        "    \"\"\"\n",
        "    Recebe mensagens do Telegram via API Gateway,\n",
        "    verifica se vêm do chat correto e salva no S3 em formato JSON.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Lê o corpo da requisição (mensagem do Telegram)\n",
        "        message = json.loads(event[\"body\"])\n",
        "        chat_id = message.get(\"message\", {}).get(\"chat\", {}).get(\"id\")\n",
        "\n",
        "        # Ignora mensagens de outros grupos\n",
        "        if chat_id != TELEGRAM_CHAT_ID:\n",
        "            logger.info(f\"Mensagem ignorada (chat_id: {chat_id})\")\n",
        "            return {\"statusCode\": 200, \"body\": \"Ignored\"}\n",
        "\n",
        "        # Cria nome de arquivo com data e hora\n",
        "        tzinfo = timezone(timedelta(hours=-3))\n",
        "        date = datetime.now(tzinfo).strftime('%Y-%m-%d')\n",
        "        timestamp = datetime.now(tzinfo).strftime('%Y%m%d%H%M%S%f')\n",
        "        key = f\"telegram/context_date={date}/{timestamp}.json\"\n",
        "\n",
        "        # Envia a mensagem diretamente pro S3\n",
        "        s3_client.put_object(\n",
        "            Bucket=BUCKET,\n",
        "            Key=key,\n",
        "            Body=json.dumps(message, ensure_ascii=False).encode(\"utf-8\")\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Mensagem salva em {key}\")\n",
        "        return {\"statusCode\": 200, \"body\": \"OK\"}\n",
        "\n",
        "    except Exception as exc:\n",
        "        logger.exception(\"Erro ao processar mensagem\")\n",
        "        return {\"statusCode\": 500, \"body\": \"Internal Server Error\"}"
      ],
      "metadata": {
        "trusted": true,
        "id": "ok8wrxc2PuZc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variaveis de ambiente\n",
        "\n",
        "Nesta etapa, serão criadas variáveis de ambiente, que são estruturas no formato chave:valor.\n",
        "Elas servem para armazenar informações sensíveis ou de configuração que a função Lambda precisa durante sua execução — sem que esses dados fiquem expostos diretamente no código-fonte.\n",
        "\n",
        "As variáveis de ambiente são definidas no ambiente de execução da função Lambda, dentro do console da AWS, na aba Configuration → Environment variables.\n",
        "\n",
        "Esse recurso é essencial para manter segurança e boas práticas, evitando a exposição de credenciais, nomes de bucket, tokens de acesso ou URLs internas.\n",
        "\n",
        "Note que o código exige a configuração de duas variáveis de ambiente: AWS_S3_BUCKET com o nome do bucket do AWS S3 e TELEGRAM_CHAT_ID com o id do chat do grupo do Telegram.\n",
        "\n",
        "Exemplo de variáveis:"
      ],
      "metadata": {
        "id": "436FI0khPuZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AWS_S3_BUCKET: BUCKET-DATALAKE-RAW\n",
        "\n",
        "TELEGRAM_CHAT_ID:-123456789"
      ],
      "metadata": {
        "id": "XLNfGM_RPuZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Permissão\n",
        "Por fim foi permitido acesso total ao  AWS S3 para a função do AWS Lambda no AWS IAM."
      ],
      "metadata": {
        "id": "idFcGy2mPuZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Minha imagem](https://github.com/WlandGLL/imagens/blob/main/110.png?raw=true)\n"
      ],
      "metadata": {
        "id": "wunYNMCyPuZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.ETL\n",
        "\n",
        "Após a fase de ingestão, em que os dados foram armazenados em formato JSON em um bucket S3, será executado um processo de ETL (Extract, Transform, Load). Nessa etapa, os arquivos JSON passam por um processo de wrangling realizado por uma função Lambda, responsável por limpar, transformar e padronizar os dados. Em seguida, os resultados são convertidos para o formato Parquet e armazenados em um bucket S3 específico, organizados em uma única pasta com orientação a colunas, visando melhor desempenho em consultas e otimização de armazenamento."
      ],
      "metadata": {
        "id": "qr5tli9iPuZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1.AWS S3\n",
        "Na etapa de ETL, o AWS S3 tem a função de passivamente armazenar as mensagens processadas de um dia em um único arquivo no formato Parquet. Para tanto, basta a criação de um bucket."
      ],
      "metadata": {
        "id": "h9yWl2ZAPuZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. AWS Lambda\n",
        "\n",
        "Agora o AWS tem a função de tratar os dados e fazer um processo de wrangling nos arquivos, após isso eles serão salvos em uma única pasta, orientada a coluna dentro de um bucket S3. Esse arquivos serão salvos no formato parquet. Para realizar todo esse processo, será criada uma função lambda.\n",
        "\n",
        "O código da função:"
      ],
      "metadata": {
        "id": "FfaXynopPuZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "from datetime import datetime, timedelta, timezone\n",
        "import boto3\n",
        "\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# FUNÇÃO AUXILIAR: Transforma o dicionário JSON em um formato plano\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def parse_data(raw_update_data: dict, date_to_process: str) -> dict:\n",
        "  \"\"\"\n",
        "  Extrai os campos relevantes da estrutura JSON da mensagem.\n",
        "  \"\"\"\n",
        "\n",
        "  # A mensagem pode estar aninhada em 'message' ou ser o objeto raiz\n",
        "  data_message = raw_update_data.get(\"message\", raw_update_data)\n",
        "\n",
        "  parsed_data = {\n",
        "    'message_id': data_message.get('message_id'),\n",
        "    'date': data_message.get('date'),\n",
        "    'text': data_message.get('text', None),\n",
        "    'user_id': data_message.get('from', {}).get('id'),\n",
        "    'user_is_bot': data_message.get('from', {}).get('is_bot', False),\n",
        "    'user_first_name': data_message.get('from', {}).get('first_name'),\n",
        "    'chat_id': data_message.get('chat', {}).get('id'),\n",
        "    'chat_type': data_message.get('chat', {}).get('type'),\n",
        "    # Coluna de partição para o S3\n",
        "    'context_date': date_to_process\n",
        "  }\n",
        "  return parsed_data\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# FUNÇÃO PRINCIPAL\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "def lambda_handler(event: dict, context: dict) -> bool:\n",
        "  # IMPORTAÇÃO LENTA (Lazy Import): Crucial para evitar o erro de 1.92 ms\n",
        "  try:\n",
        "    import awswrangler as wr\n",
        "    import pandas as pd # Adicionado pandas para criar o DataFrame\n",
        "    logger.info(\"[INFO] awswrangler e pandas importados com sucesso.\")\n",
        "  except Exception as exc:\n",
        "    logger.error(f\"[ERRO CRÍTICO] Falha ao importar bibliotecas: {exc}\", exc_info=True)\n",
        "    return False\n",
        "\n",
        "\n",
        "  logger.info(\"--- INICIANDO ETL com duas funções ---\")\n",
        "\n",
        "\n",
        "  # 1. Leitura das Variáveis de Ambiente\n",
        "  try:\n",
        "    RAW_BUCKET = os.environ['AWS_S3_BUCKET']\n",
        "    ENRICHED_BUCKET = os.environ['AWS_S3_ENRICHED']\n",
        "  except KeyError as exc:\n",
        "    logger.error(f\"Erro de Variável de Ambiente: {exc}\")\n",
        "    return False\n",
        "\n",
        "\n",
        "\n",
        "  # 2. Definição da Data de Processamento (Lógica Corrigida)\n",
        "  tzinfo = timezone(offset=timedelta(hours=-3))\n",
        "  date_to_process = event.get(\"date\")\n",
        "\n",
        "\n",
        "  if not date_to_process:\n",
        "    # date_to_process = (datetime.now(tzinfo) - timedelta(days=1)).strftime('%Y-%m-%d')\n",
        "    date_to_process = (datetime.now(tzinfo) - timedelta(days=0)).strftime('%Y-%m-%d')\n",
        "\n",
        "  logger.info(f\"Processando dados para a partição: {date_to_process}\")\n",
        "\n",
        "\n",
        "  # 3. Listar e processar arquivos JSON\n",
        "  s3_path_prefix = f'telegram/context_date={date_to_process}'\n",
        "  client = boto3.client('s3')\n",
        "  lista_de_mensagens = []\n",
        "\n",
        "\n",
        "  try:\n",
        "    response = client.list_objects_v2(\n",
        "      Bucket=RAW_BUCKET,\n",
        "      Prefix=s3_path_prefix\n",
        "    )\n",
        "\n",
        "\n",
        "    if 'Contents' not in response:\n",
        "      logger.warning(f\"Nenhum arquivo JSON encontrado para {date_to_process}\")\n",
        "      return False\n",
        "\n",
        "\n",
        "    for content in response['Contents']:\n",
        "      key = content['Key']\n",
        "      file_name = key.split('/')[-1]\n",
        "      tmp_path = f\"/tmp/{file_name}\"\n",
        "\n",
        "\n",
        "      try:\n",
        "        # Baixa e lê o arquivo JSON\n",
        "        client.download_file(RAW_BUCKET, key, tmp_path)\n",
        "        with open(tmp_path, mode='r', encoding='utf8') as fp:\n",
        "          raw_update_data = json.load(fp)\n",
        "\n",
        "\n",
        "        # Usa a função auxiliar para extrair os dados\n",
        "        parsed_data = parse_data(raw_update_data, date_to_process)\n",
        "        lista_de_mensagens.append(parsed_data)\n",
        "\n",
        "      except Exception as exc:\n",
        "        logger.error(f\"Erro ao processar arquivo {key}. Pulando. Detalhe: {exc}\", exc_info=True)\n",
        "        continue\n",
        "\n",
        "\n",
        "    # 4. Converte e Salva em Parquet\n",
        "    if lista_de_mensagens:\n",
        "      df = pd.DataFrame(lista_de_mensagens)\n",
        "\n",
        "      s3_path_write = f's3://{ENRICHED_BUCKET}/telegram_enriched/'\n",
        "\n",
        "      # Escreve o DataFrame como Parquet no S3\n",
        "      wr.s3.to_parquet(\n",
        "        df=df,\n",
        "        path=s3_path_write,\n",
        "        dataset=True,\n",
        "        partition_cols=['context_date'],\n",
        "        mode=\"append\"\n",
        "      )\n",
        "\n",
        "      logger.info(f\"ETL concluído. {len(df)} linhas salvas em Parquet.\")\n",
        "      return True\n",
        "    else:\n",
        "      logger.warning(f\"Nenhum dado processado para {date_to_process}\")\n",
        "      return False\n",
        "\n",
        "\n",
        "  except Exception as exc:\n",
        "    logger.error(f\"Erro fatal: {exc}\", exc_info=True)\n",
        "    return False"
      ],
      "metadata": {
        "trusted": true,
        "id": "s9QdkqcQPuZj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variaveis de ambiente\n",
        "\n",
        "Nesta etapa, serão criadas variáveis de ambiente, que são estruturas no formato chave:valor.\n",
        "Elas servem para armazenar informações sensíveis ou de configuração que a função Lambda precisa durante sua execução — sem que esses dados fiquem expostos diretamente no código-fonte.\n",
        "\n",
        "As variáveis de ambiente são definidas no ambiente de execução da função Lambda, dentro do console da AWS, na aba Configuration → Environment variables.\n",
        "\n",
        "Esse recurso é essencial para manter segurança e boas práticas, evitando a exposição de credenciais, nomes de bucket, tokens de acesso ou URLs internas.\n",
        "\n",
        "Note que o código exige a configuração de duas variáveis de ambiente: AWS_S3_BUCKET com o nome do bucket com os arquivos json  e AWS_S3_ENRICHED com o nome do bucket aonde serão salvos os arquivos parquet.\n",
        "\n"
      ],
      "metadata": {
        "id": "aq4w2cVJPuZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Permissão\n",
        "Por fim foi permitido acesso total ao  AWS S3 para a função do AWS Lambda no AWS IAM.\n",
        "\n",
        "![Minha imagem](https://github.com/WlandGLL/imagens/blob/main/114.png?raw=true)\n"
      ],
      "metadata": {
        "id": "W51SMxECPuZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3. AWS Event Bridge\n",
        "Na etapa de ETL, o AWS Event Bridge tem a função de ativar diariamente a função de ETL do AWS Lambda, funcionando assim como um scheduler."
      ],
      "metadata": {
        "id": "J1hy9jkiPuZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Apresentação\n",
        "Nesta etapa, os dados ficam disponibilizados tanto para os usuários finais — como analistas e cientistas de dados — quanto para sistemas de visualização e consulta, como dashboards e mecanismos de busca. A principal forma de acesso a essas informações se dá por meio de ferramentas de consulta, especialmente SQL, que é amplamente utilizada por grande parte dos usuários. Nesse cenário, utilizanos AWS Athena é empregado como solução de leitura, oferecendo um motor de consulta SQL que facilita a exploração e visualização dos dados já processados na camada ETL, promovendo análises mais eficientes e acessíveis."
      ],
      "metadata": {
        "id": "e4ZdOeC8PuZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 AWS Athena\n",
        "Na etapa da apresentação o AWS Athena tem a função de apresentar os dados, no formato de tabela em sql. Para isso, vamos usar um código em sql para criar a tabela"
      ],
      "metadata": {
        "id": "f8oZgr_CPuZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Minha imagem](https://github.com/WlandGLL/imagens/blob/main/115.png?raw=true)\n"
      ],
      "metadata": {
        "id": "_EU1Iz4PPuZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Toda vez que uma nova partição é adicionada ao repositório de dados, é necessário informar o AWS Athena para que a ela esteja disponível via SQL. Para isso, usamos o comando SQL abaxio:"
      ],
      "metadata": {
        "id": "jv8b3AbZPuZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Minha imagem](https://github.com/WlandGLL/imagens/blob/main/116.png?raw=true)\n"
      ],
      "metadata": {
        "id": "aY2WRIQFPuZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por fim, é feito um consulta para verififcar se a tabela foi criada com êxito\n",
        "\n",
        "![Minha imagem](https://github.com/WlandGLL/imagens/blob/main/117.png?raw=true)\n"
      ],
      "metadata": {
        "id": "V5YSE3xIPuZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resposta:\n",
        "\n",
        "![Minha imagem](https://github.com/WlandGLL/imagens/blob/main/118.png?raw=true)\n"
      ],
      "metadata": {
        "id": "96l3FfVtPuZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2. Análise Exploratória\n",
        "Com os dados disponíveis, podemos executar as mais variadas consultas analíticas. Mas por se tratar de um teste inicial que compreendeu apenas o perído de 30-10 até 06-11-2025, teríamos que continuar acompanhando a frequência de mensagens num período maior para assim conseguirmos chegar a uma melhor análise de dados.\n",
        "\n",
        "Seguem abaixo alguns exemplos de consultas SQL, seus resultados e alguns gráficos gerados para a melhor visualização dos resultados:"
      ],
      "metadata": {
        "id": "sxGcB_FCPuZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Quantas mensagens foram enviadas por dia?**"
      ],
      "metadata": {
        "id": "JSZtaZOiPuZo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Minha imagem](https://github.com/WlandGLL/imagens/blob/main/119.png?raw=true)\n",
        "\n",
        "### **Resposta:**\n",
        "\n",
        "![Minha imagem](https://github.com/WlandGLL/imagens/blob/main/120.png?raw=true)\n",
        "\n"
      ],
      "metadata": {
        "id": "1phq0fRZPuZo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Houve mensagens repitidas?**\n",
        "\n",
        "![Minha imagem](https://github.com/WlandGLL/imagens/blob/main/121.png?raw=true)\n",
        "\n",
        "\n",
        "### **Resposta:**\n",
        "\n",
        "![Minha imagem](https://github.com/WlandGLL/imagens/blob/main/122.png?raw=true)\n"
      ],
      "metadata": {
        "id": "gx0b82GHPuZo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gráfico com a quantidade de mensagens por usuário**\n",
        "\n"
      ],
      "metadata": {
        "id": "cm8v-NBbPuZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "mensagens = pd.read_csv('/kaggle/input/mensagensusuario/other/default/1/mensagensporusuario.csv')"
      ],
      "metadata": {
        "trusted": true,
        "id": "XJeenf6zPuZp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(\n",
        "    data=mensagens,\n",
        "    x='nome_usuario',\n",
        "    y='qut_mensagem',\n",
        "    palette='viridis'\n",
        ")\n",
        "plt.title('Quantidade de mensagens por usuário', fontsize=14, weight='bold')\n",
        "plt.xlabel('Usuário', fontsize=12)\n",
        "plt.ylabel('Quantidade de Mensagens', fontsize=12)"
      ],
      "metadata": {
        "trusted": true,
        "id": "tlnK7PTePuZp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Minha imagem](https://github.com/WlandGLL/imagens/blob/main/123.png?raw=true)\n"
      ],
      "metadata": {
        "id": "2_iW5zPSPuZq"
      }
    }
  ]
}